# Maximum likelihood estimation for ERGMs {#MLE_ERGMs}

The estimation procedure starts assuming that the observed network $y$ is just one realization of a set of many possible networks that could have been formed by the same underlying structural processes. Each network statistic $s(y)$ is conceptualised as having a particular probability of occurring which is incorporated into the model as a parameter vector $\theta$.

The aim of the estimation is to find accurately a parameter value so that the observed network has the highest possible likelihood of being simulated by the given model. In other words, when attempting to generate an ERGM we find the set of parameters which maximise the probability that any random network graph generated by simulating from the ERGM will be identical to the observed network in terms of network statistics:
\begin{equation*}
\mathbb{E}_{y\ |\ \theta}\left[s(y)\right] = s(y)
\end{equation*}
where $\mathbb{E}_{y\ |\ \theta}$ is the expectation under the likelihood distribution $p(y\ |\ \theta)$ so that it ensures that the probability mass of the ERGM is centred at $s(y)$.

In practice, an exact solution for the ERGM is actually impossible to calculate directly due to the **intractability** of the normalising constant $c(\theta)$.



## Monte Carlo maximum likelihood estimation (MC-MLE)

The most advanced classical estimation method used is the Monte Carlo Maximum Likelihood Estimation (MC-MLE) procedure proposed by [Geyer and Thompson (1992)](http://www.jstor.org/stable/2345852). 

This involves the simulation of a set of random networks from a starting set of estimated parameter values $\theta_0$, and then the progressive refinement (until convergence) of the parameter values by measuring how closely these networks match the observed network.

Let's denote with $q_{\theta}(y) = \exp\{ \theta^t s(y) \}$ the unnormalised likelihood. Mathematically, the key identity of the MC-MLE method is the following:
\begin{align}
\frac {c(\theta)} {c(\theta_0)} 
= \mathbb{E}_{y\ |\ \theta_0} 
\left[ 
\frac {q_{\theta}(y)} {q_{\theta_0}(y)} 
\right] 
&= \sum_{y} 
\frac {q_{\theta}(y)} {q_{\theta_0}(y)} 
\frac {q_{\theta_0}(y)} {z(\theta_0)} \label{eqn:key}
\\
&\approx \frac{1}{m}\sum_{i=1}^m \exp\left\{ (\theta-\theta_0)^t s(y_i) \right\}\notag
\end{align}
where $q(\cdot)$ is the unnormalised likelihood, $\theta_0$ is fixed set of parameter 
values, and $\mathbb{E}_{y\ |\ \theta_0}$ denotes an expectation taken with respect to the 
distribution $p(y\ |\ \theta_0)$. In practice this ratio of normalising constants is 
approximated using graphs $y_1,\dots,y_m$  sampled via MCMC from $p(y\ |\ \theta_0)$ 
and importance sampling. 

This yields the following approximated log likelihood ratio:
\begin{equation} 
w_{\theta_0}(\theta)=\ell(\theta)-\ell(\theta_0) \approx (\theta - \theta_0)^t s(y) - 
\log\left[ \frac{1}{m}\sum_{i=1}^m \exp\left\{ (\theta-\theta_0)^t s(y_i) \right\} \right] 
\label{eqn:gey:thom}
\end{equation}
where $\ell(\cdot)$ is the log-likelihood. $w_{\theta_0}$ is a function of $\theta$, and 
its maximum value serves as a Monte Carlo estimate of the MLE.

A crucial aspect of this algorithm is the choice of $\theta_0$. Ideally $\theta_0$ should be very close to the maximum likelihood estimator of $\theta$. Viewed as a function of $\theta$, $w_{\theta_0}(\theta)$ is very sensitive to the choice of $\theta_0$.
A poorly chosen value of $\theta_0$ may lead to an objective function that cannot even be maximised.

The result of the MC-MLE procedure is a set of estimated parameter values and relative standard errors which measure the degree of accuracy and significance of the estimates.

To implement the MC-MLE procedure we can use the `ergm` function: 
```{r}
set.seed(11)
model.2 <- y ~ edges + 
               gwesp(decay = 1, fixed = TRUE) + 
               gwnsp(decay = 1, fixed = TRUE)
MLE.2 <- ergm(model.2)
summary(MLE.2)
```

```{r, message=FALSE}
mcmc.diagnostics(MLE.2)
```



## Goodness of fit diagnostics {.tabset .tabset-fade}

If the estimated ERGM is a good fit to the observed data, then networks simulated $y_1, \dots, y_m$ from its MLE should resemble the connectivity structure of the observed data $y$. 

To do this, $m = 100$ graphs are simulated from the maximum likelihood estimate of the parameter vector $\hat{\theta}$ and compared to the observed graph in terms of high-level network statistics which are not modelled explicitly:
```{r}
model.2.gof <- gof(MLE.2 ~ degree + esp + distance,
                   control.gof.formula(nsim = 100))
```

### Graphical diagnostics
```{r}
par(mfrow = c(1, 3))
plot(model.2.gof, main = '')
```

### Summaries
```{r}
summary(model.2.gof)
```




