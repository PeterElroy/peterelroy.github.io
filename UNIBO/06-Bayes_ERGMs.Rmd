---
output:
  pdf_document: default
  html_document: default
---
# Bayesian inference for ERGMs {#Bayes_ERGMs}

## Parameter inference via approximate exchange algorithm

Bayesian inference for ERGMs is based on the posterior distribution of $\theta$ given the data $y$:
$$
p(\theta | y) 
= p(y | \theta)\; \frac{p(\theta)}{p(y)} 
= \frac{ \exp \{ \theta^t s(y)\} }
       {z(\theta)}\; \frac{p(\theta)}{p(y)},
$$
where $p(\theta)$ is the prior parameter distribution and $p(y)$ is the evidence or marginal likelihood of $y$ which is typically intractable. 

Standard MCMC methods such as the Metropolis-Hastings algorithm, can deal with posterior estimation as long as the target posterior density is known up to the model evidence $p(y)$. Unfortunately in the ERGM context the posterior density $p(\theta | y)$  of non-trivially small ERGMs includes two intractable normalising constants, the model evidence $p(y)$ and $z(\theta)$. For this reason, the ERGM posterior density is **doubly intractable**.

In order to carry out Bayesian inference for ERGMs, the `Bergm` package makes use of MCMC techniques [@cai:fri11]. The *approximate exchange algorithm* circumvents the problem of computing the normalising constants of the ERGM likelihoods, while the use of multiple chains and efficient adaptive proposal strategies are able to speed up the computations and improve chain mixing quite significantly.

Let's denote $q_{\theta}(y) = \exp \{ \theta^t s(y) \}$ the unnormalised likelihood. The approximate exchange algorithm implemented by the `bergm` function can be summarised in the following way:

1. Gibbs update of $(\theta',y')$

$\qquad$i) Draw $\theta' \sim h(\cdot|\theta)$
   
  ii) Approximately draw $y' \sim f(\cdot|\theta')$ via MCMC

2. Accept move from $\theta$ to $\theta'$ with probability:
			$$
			1 \wedge \frac{q_{\theta'}(y)}{q_{\theta}(y)}
			\frac{p(\theta')}{p(\theta)} 
			\frac{h(\theta|\theta')}{h(\theta'|\theta)}
			\frac{q_{\theta}(y')}{q_{\theta'}(y')} 
			\times \underbrace{\frac{z(\theta)z(\theta')}{z(\theta')z(\theta)}}_{1}.
			$$	                           

### Parallel adaptive direction sampler

In order to improve mixing a parallel adaptive direction sampler (ADS) [@gil:rob:geo94; @rob:gil94] is considered: at the $i$-th iteration of the algorithm 
we have a collection of $H$ different chains interacting with one another. By construction, the state space consists of 
$\{\theta_1,\dots,\theta_H\}$ with target distribution $p(\theta_1|y)\otimes\dots\otimes p(\theta_H | y)$. A 
parallel ADS move consists of generating a new value $\theta'_h$ from the difference of two parameters $\theta_{h_1}$ and 
$\theta_{h_2}$  (randomly selected from other chains) multiplied by a scalar term $\gamma$ which is called parallel ADS move 
factor plus a random term $\epsilon$ called parallel ADS move parameter.

Consider the Zachary Karate club social network consisting of social relations in a university karate club involving 34 individuals. The nodal covariate `faction.id`
is encoding the faction alignment coded numerically, as -2 (strongly Mr. Hi's), -1 (weakly Mr. Hi's), 0 (neutral), +1 (weakly John's), and +2 (strongly John's):
```{r, fig.height=3, fig.width=4, message=FALSE}
library(Bergm); library(statnet)
data(zach)
y <- zach
y.node.col <- rainbow(6)[y %v% "faction.id" + 3]
set.seed(11)
plot(y, vertex.col = y.node.col, vertex.cex = 1.3)
legend("topleft",  
       pch = 16, 
       col = rainbow(6)[sort(unique(y %v% "faction.id") + 3)], 
       legend = c("strongly Mr. Hi's", 
                  "weakly Mr. Hi's", 
                  "neutral", 
                  "weakly John's", 
                  "strongly John's"), 
       title = 'Faction')
```

and consider the following model:
```{r, message=FALSE}
model.3 <- y ~ edges + 
               nodematch("faction.id") +
               gwesp(decay = log(2), fixed = TRUE)
```

## Prior specification

We can also set up the prior mean and variance/covariance structure. 
The prior distribution used by the `Bergm` package is the normal distribution. In this case, we set the prior mean to be $\bar{\theta} = (-1, 0.5, 0)$ (corresponding to negative density and positive density withing factions and positive transitivity) and the covariance matrix of each model to be a diagonal matrix with every entry equal to $4$.
So 
$$
\theta \sim N \left(\begin{bmatrix}
-1\\ 
0.5\\ 
0.5
\end{bmatrix},
\begin{bmatrix}
4 & 0 & 0\\ 
0 & 4 & 0\\ 
0 & 0 & 4
\end{bmatrix}
\right)
$$
```{r}
mean.prior <- c(-1, 0.5, 0.5)
sigma.prior <- diag(4, 3)
```

To implement the Bayesian parameter inference procedure we can use the `bergm` function:
```{r, message=FALSE}
post <- bergm(model.3, 
              aux.iters = 2000, # number of iterations for step 1.ii
              burn.in = 100, # number of burnin iterations for each chain
              main.iters = 3000, # number of iterations for each chain (after burnin)
              nchains = 5, # number of chains
              gamma = 0.8, # gamma parameter (tuned to get ~20% acceptance probability)
              mean.prior = mean.prior,
              sigma.prior = sigma.prior)
```

It is possible to visualise the results of the MCMC estimation by using the `bergm.output` function which is based on the `coda` package:
```{r}
bergm.output(post)
```

## Bayesian goodness of fit diagnostics {.tabset .tabset-fade}

The `bgof` function provides a useful tool for assessing Bayesian goodness-of-fit so as to examine the fit of the data to the posterior model obtained by the `bergm` function. The observed network data $y$ are compared with a set of networks $y_1, y_2, \dots, y_S$ simulated from $S$ independent realisations $\theta_1, \theta_2,\dots,\theta_S$ of the posterior density estimate. This comparison is made in terms of high-level characteristics $g(\cdot)$ such as higher degree distributions, etc. [@hun:goo:han08].

```{r}
bgof(post, aux.iters = 5000, n.deg = 15, n.dist = 9, n.esp = 8)
```

The red line displays the goodness of fit statistics for the observed data together with boxplots of goodness of fit statistics based on $100$ simulated networks from the estimated posterior distribution.



